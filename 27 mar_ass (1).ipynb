{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48880404-fb3e-42ba-b00c-3643008dab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406d8fe-f085-4125-b749-15a716aaa8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared, also known as the coefficient of determination, is a statistical measure that is commonly used to evaluate the goodness of fit of a linear regression model. \n",
    "It provides an indication of how well the regression model represents the data points.\n",
    "\n",
    "R-squared is a value between 0 and 1, where 0 indicates that the model does not explain any of the variability in the response variable, and 1 indicates that the model \n",
    "explains all of the variability in the response variable. In other words, it measures the proportion of the variance in the dependent variable that can be explained by the\n",
    "independent variables in the regression model.\n",
    "\n",
    "To calculate R-squared, we compare the total sum of squares (TSS) and the residual sum of squares (RSS). The TSS represents the total variability in the dependent variable, \n",
    "while the RSS represents the unexplained variability or the sum of squared residuals. The formula for R-squared is as follows:\n",
    "\n",
    "R-squared = 1 - (RSS / TSS)\n",
    "\n",
    "where:\n",
    "\n",
    "RSS is the sum of squared residuals, which is the sum of the squared differences between the actual values of the dependent variable and the predicted values from the \n",
    "regression model.\n",
    "TSS is the total sum of squares, which is the sum of the squared differences between the actual values of the dependent variable and the mean value of the dependent variable.\n",
    "By dividing the RSS by the TSS and subtracting it from 1, we obtain R-squared. If the model perfectly predicts the dependent variable, the RSS will be zero, resulting in an\n",
    "R-squared value of 1. However, if the model does not provide any improvement over using the mean value of the dependent variable as a predictor, the RSS will be equal to the \n",
    "TSS, resulting in an R-squared value of 0.\n",
    "\n",
    "R-squared is an important metric in regression analysis as it helps assess the goodness of fit of the model. However, it does have limitations. R-squared does not indicate \n",
    "whether the regression model is statistically significant or if it has the correct functional form. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b359b78-2111-4e43-9165-45f1416b75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8cb0c-5311-4d28-b338-64845f051672",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables in a linear regression model. \n",
    "While R-squared measures the proportion of the variance in the dependent variable explained by the independent variables, adjusted R-squared adjusts for the number of\n",
    "predictors in the model to provide a more reliable assessment of model fit.\n",
    "\n",
    "The formula for adjusted R-squared is as follows:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "where:\n",
    "\n",
    "R-squared is the regular coefficient of determination.\n",
    "n is the number of observations or data points.\n",
    "k is the number of independent variables or predictors in the model.\n",
    "The key difference between adjusted R-squared and R-squared lies in the penalty applied to the R-squared value based on the number of predictors and observations. \n",
    "Adjusted R-squared penalizes excessive inclusion of independent variables that may not improve the model's explanatory power. It accounts for the possibility that adding\n",
    "more predictors to the model may increase the R-squared value by chance or due to overfitting.\n",
    "\n",
    "By including a penalty term that increases with the number of predictors relative to the number of observations, adjusted R-squared tends to decrease when irrelevant or \n",
    "weakly relevant predictors are added to the model. This adjustment provides a more conservative and accurate assessment of how well the independent variables explain the\n",
    "dependent variable's variability.\n",
    "\n",
    "Compared to R-squared, adjusted R-squared is generally a more appropriate measure when comparing models with different numbers of predictors. It helps prevent the \n",
    "overestimation of model performance and facilitates model selection by prioritizing simplicity and avoiding the inclusion of unnecessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879136f7-d16e-4c85-9515-5a712ff6dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2270bf7-603d-4d62-9ccd-e2bc49b6c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is more appropriate to use in situations where you want to compare the performance of regression models with different numbers of predictors or when you \n",
    "want to prioritize model simplicity.\n",
    "\n",
    "Here are some specific scenarios where adjusted R-squared is particularly useful:\n",
    "\n",
    "Model comparison: When you have multiple regression models with different numbers of predictors, comparing their adjusted R-squared values can help you determine which model \n",
    "provides a better balance between explanatory power and simplicity. Models with higher adjusted R-squared values are generally preferred as they better capture the \n",
    "relationship between the independent variables and the dependent variable while accounting for the number of predictors.\n",
    "\n",
    "Feature selection: Adjusted R-squared can be helpful in feature selection or variable elimination processes. It penalizes the addition of irrelevant or weakly relevant \n",
    "predictors, encouraging the exclusion of variables that do not significantly contribute to the model's explanatory power. By comparing adjusted R-squared values, you can \n",
    "identify a subset of predictors that collectively provide the best fit for the data.\n",
    "\n",
    "Overfitting prevention: Adjusted R-squared serves as a tool to guard against overfitting, which occurs when a model performs well on the training data but poorly on new, \n",
    "unseen data. By penalizing models with an excessive number of predictors, adjusted R-squared helps select models that are less likely to be overfitted. It encourages\n",
    "parsimony, favoring simpler models that generalize better to new data.\n",
    "\n",
    "Sample size limitations: Adjusted R-squared becomes particularly useful when the sample size is small relative to the number of predictors. In such cases, regular R-squared\n",
    "may increase even with the addition of weak predictors due to chance or overfitting. Adjusted R-squared provides a more conservative estimate of model fit by accounting for \n",
    "the limited amount of data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ff478-86c3-4bc4-af07-77853c3275f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f5892-42ff-4ac7-970b-59c15eb4544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the performance of a regression model and measure the accuracy of its predictions. \n",
    "These metrics quantify the differences between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "RMSE is a widely used metric that measures the average magnitude of the residuals (the differences between predicted and actual values) in the original units of the dependent variable. It provides an indication of how well the model's predictions fit the actual data points. The formula for RMSE is as follows:\n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "where MSE is the Mean Squared Error.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE measures the average squared difference between the predicted values and the actual values of the dependent variable. It gives higher weight to larger errors and penalizes larger deviations more than MAE. The formula for MSE is as follows:\n",
    "MSE = (1/n) * Σ(yᵢ - ŷᵢ)²\n",
    "\n",
    "where n is the number of data points, yᵢ represents the actual values, and ŷᵢ represents the predicted values.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE is another widely used metric that measures the average absolute difference between the predicted values and the actual values. It provides a measure of the average\n",
    "magnitude of errors without considering their direction. The formula for MAE is as follows:\n",
    "MAE = (1/n) * Σ|yᵢ - ŷᵢ|\n",
    "\n",
    "where n is the number of data points, yᵢ represents the actual values, and ŷᵢ represents the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721bde2-0b68-4303-878c-542763168a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a1107-61e6-42ae-b5d0-f89f5298dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Advantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "Straightforward Interpretation: RMSE, MSE, and MAE provide intuitive and easily interpretable measures of the prediction errors in the original units of the dependent \n",
    "variable. This makes it easier to understand the magnitude and scale of the errors.\n",
    "\n",
    "Sensitivity to Large Errors: RMSE and MSE both give higher weight to larger errors due to the squaring operation, which can be beneficial when large errors are of particular\n",
    "concern or need to be penalized more heavily.\n",
    "\n",
    "Commonly Used: RMSE, MSE, and MAE are widely used and accepted metrics in regression analysis. They are well-established and familiar to researchers, making it easier to \n",
    "compare and communicate results across studies.\n",
    "\n",
    "Disadvantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "Lack of Robustness to Outliers: RMSE and MSE can be heavily influenced by outliers due to the squaring operation, as they increase the magnitude of the errors. In the \n",
    "presence of outliers, these metrics may not accurately represent the overall model performance and can be skewed by a few extreme values.\n",
    "\n",
    "Sensitivity to Scale: RMSE and MSE are sensitive to the scale of the dependent variable. If the scale of the dependent variable is large, the resulting RMSE and MSE values \n",
    "will also be large, making it difficult to compare the performance across different datasets or models.\n",
    "\n",
    "Directionless Errors: MAE does not consider the direction of errors, treating overestimation and underestimation equally. While this can be advantageous in certain cases, \n",
    "it may also mask potential systematic biases in the model's predictions.\n",
    "\n",
    "Limited Information: RMSE, MSE, and MAE provide only a summary measure of the overall prediction accuracy. They do not provide insights into specific patterns of errors or\n",
    "how well the model performs across different regions of the data space. Additional diagnostic tools and visualization techniques may be required to gain a more comprehensive\n",
    "understanding of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed82a0-e27e-491a-a250-0c8497b05d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd5cda-30f3-4816-a3fb-794c331b1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in regression analysis to reduce the complexity of a model and prevent overfitting. It adds a penalty term to the regression objective function, which encourages the model to select a subset of the most relevant features by shrinking the coefficients of less important predictors to zero.\n",
    "\n",
    "In Lasso regularization, the penalty term is based on the sum of the absolute values of the regression coefficients multiplied by a tuning parameter (λ). The objective function in Lasso regression is defined as follows:\n",
    "\n",
    "Objective = RSS + λ * Σ|β|\n",
    "\n",
    "where:\n",
    "\n",
    "RSS is the residual sum of squares, which measures the discrepancy between the predicted and actual values.\n",
    "Σ|β| is the sum of the absolute values of the regression coefficients.\n",
    "λ is the regularization parameter that controls the strength of the penalty.\n",
    "The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term. While Lasso uses the sum of the absolute values of the coefficients, Ridge regularization uses the sum of the squared values of the coefficients. This difference has implications for the shrinkage effect on the coefficients.\n",
    "\n",
    "The effects of Lasso regularization include:\n",
    "Feature Selection: Lasso has a tendency to drive the coefficients of irrelevant or weakly relevant predictors to exactly zero. This property makes it useful for feature selection, as it automatically identifies and discards less important predictors from the model.\n",
    "Sparsity: Lasso produces sparse solutions, meaning that it leads to models with fewer non-zero coefficients. This can improve the interpretability of the model and reduce the risk of overfitting, especially when dealing with high-dimensional data or when there is a large number of potentially irrelevant predictors.\n",
    "Model Simplicity: Lasso regularization encourages model simplicity by favoring a smaller number of predictors. This can help avoid overfitting, improve generalization to new data, and facilitate model interpretation.\n",
    "\n",
    "When to use Lasso regularization:\n",
    "Lasso regularization is more appropriate when:\n",
    "\n",
    "There is a large number of predictors, especially when many of them are potentially irrelevant.\n",
    "Feature selection is desired, and the goal is to identify the most important predictors while discarding the less relevant ones.\n",
    "Model interpretability and simplicity are important considerations.\n",
    "The assumption of independent and identically distributed errors is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b0dd9-da6c-464a-b16c-4fadaf1991b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f5d3f-0f91-45b2-904f-26f76134efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by adding a penalty term to the regression objective \n",
    "function. This penalty term discourages the model from relying too heavily on any individual predictor or from fitting noise in the training data. By controlling the \n",
    "complexity of the model, regularized linear models can improve generalization to new, unseen data and mitigate the risk of overfitting.\n",
    "\n",
    "Let's consider an example where we have a dataset with a single independent variable (X) and a dependent variable (y). We want to fit a linear regression model to the data \n",
    "to predict y based on X. However, the dataset contains some random noise, and we are concerned about the model overfitting to this noise.\n",
    "\n",
    "Linear Regression:\n",
    "In regular linear regression, the model tries to minimize the sum of squared residuals (RSS) between the predicted values and the actual values. It aims to find the line\n",
    "that best fits the training data. However, if we have a limited amount of data points or noise in the data, the model may overfit by capturing the noise instead of the \n",
    "underlying trend.\n",
    "\n",
    "Ridge Regression:\n",
    "To address overfitting, we can use Ridge regression, which introduces a penalty term based on the sum of squared regression coefficients multiplied by a regularization \n",
    "parameter (λ). This penalty term is added to the RSS in the objective function.\n",
    "\n",
    "Ridge regression encourages the model to shrink the coefficients, reducing their impact on the predictions. This helps to prevent overfitting by controlling the complexity\n",
    "of the model. The larger the value of λ, the greater the amount of shrinkage applied to the coefficients.\n",
    "\n",
    "By tuning the value of λ, we can strike a balance between fitting the training data and avoiding overfitting. Ridge regression can also handle multicollinearity between \n",
    "predictors by shrinking correlated coefficients together.\n",
    "\n",
    "Lasso Regression:\n",
    "Similarly, Lasso regression also addresses overfitting by adding a penalty term to the objective function. However, in Lasso regression, the penalty term is based on the \n",
    "sum of the absolute values of the regression coefficients multiplied by a regularization parameter (λ).\n",
    "Lasso regression not only shrinks the coefficients but also has the ability to set some coefficients exactly to zero. This makes it useful for feature selection, as it \n",
    "automatically selects the most important predictors and discards the less relevant ones.\n",
    "\n",
    "By controlling the value of λ, we can control the amount of regularization and prevent overfitting by favoring a simpler model with fewer predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcad23-a3da-40a8-aa52-42a835629ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f24e3-8815-4283-9b59-29d5c11272a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "While regularized linear models, such as Ridge regression and Lasso regression, are powerful techniques for addressing overfitting and improving generalization, they also\n",
    "have limitations that may make them less suitable in certain situations. Here are some limitations to consider:\n",
    "\n",
    "Interpretability: Regularized linear models can reduce the interpretability of the model compared to standard linear regression. As the penalty terms introduce bias towards\n",
    "simpler models and shrink the coefficients, it becomes more challenging to directly interpret the magnitudes and signs of the coefficients. In situations where interpretability is a critical requirement, standard linear regression might be preferred.\n",
    "\n",
    "Feature Selection Challenges: While Lasso regression is known for its ability to perform feature selection by driving some coefficients to exactly zero, it may struggle in \n",
    "situations with highly correlated predictors (multicollinearity). Lasso tends to arbitrarily select one of the correlated predictors while shrinking the others, which can \n",
    "lead to instability in feature selection results. In such cases, Ridge regression or other techniques like Elastic Net might be more appropriate.\n",
    "\n",
    "Parameter Tuning: Regularized linear models require the tuning of the regularization parameter (λ) to control the degree of regularization. Selecting an optimal value for\n",
    "λ can be challenging and may require cross-validation or other techniques. If the regularization parameter is not appropriately chosen, the model's performance may suffer,\n",
    "leading to underfitting or overfitting.\n",
    "\n",
    "Sensitivity to Data Scaling: Regularized linear models are sensitive to the scale of the predictors. If the predictors have different scales, the regularization penalty \n",
    "may be applied unevenly, and the model's performance can be affected. It is essential to scale the predictors properly before applying regularization to ensure fair treatment of all features.\n",
    "\n",
    "Nonlinear Relationships: Regularized linear models assume a linear relationship between the predictors and the dependent variable. If the relationship is highly nonlinear, \n",
    "using regularized linear models may result in a poor fit and limited predictive accuracy. In such cases, nonlinear regression techniques or other machine learning algorithms\n",
    "that can capture nonlinear relationships may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c41f5-65d5-4c44-a1db-0d61d4f0ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b41d60-3468-45b6-8a9a-e05c34db4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model A has an RMSE (Root Mean Squared Error) of 10, indicating that, on average, its predictions deviate from the actual values by approximately 10 units in the original\n",
    "scale of the dependent variable.\n",
    "\n",
    "Model B has an MAE (Mean Absolute Error) of 8, which indicates that, on average, its predictions deviate from the actual values by approximately 8 units.\n",
    "\n",
    "In this scenario, a lower value of the evaluation metric suggests better performance. Therefore, Model B, with a lower MAE of 8, may be considered the better performer in \n",
    "terms of prediction accuracy.\n",
    "\n",
    "However, it's important to note the limitations of the chosen metric. While MAE provides a direct measure of the average magnitude of errors, it does not consider the\n",
    "direction of the errors. On the other hand, RMSE considers both the magnitude and direction of errors. By squaring the errors, RMSE gives more weight to larger errors \n",
    "compared to MAE. If the specific context prioritizes larger errors more heavily, such as in cases where outliers or extreme values are of significant concern, RMSE might\n",
    "provide a more appropriate measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6104c-5685-43df-ab2e-2a682cdaad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "# regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "# uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "# better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "# method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8580bc-b639-415b-9f4d-fee39cdc3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A uses Ridge regularization with a regularization parameter of 0.1. Ridge regularization adds a penalty term based on the sum of squared coefficients to the objective\n",
    "# function. The regularization parameter controls the strength of the penalty, and a lower value like 0.1 indicates a relatively weaker penalty.\n",
    "\n",
    "# Model B uses Lasso regularization with a regularization parameter of 0.5. Lasso regularization adds a penalty term based on the sum of absolute values of the coefficients. \n",
    "# The regularization parameter also controls the strength of the penalty, and a higher value like 0.5 indicates a stronger penalty.\n",
    "\n",
    "# In terms of the regularization strength, Model B with Lasso regularization and a higher regularization parameter of 0.5 may be considered to have a stronger regularization\n",
    "# effect compared to Model A with Ridge regularization and a lower regularization parameter of 0.1.\n",
    "\n",
    "# The choice of the better performer depends on the specific goals of the analysis. If the objective is to prioritize feature selection and obtain a sparser model, Lasso \n",
    "# regularization (Model B) may be preferred. Lasso has the ability to set some coefficients exactly to zero, automatically performing feature selection and retaining only the \n",
    "# most important predictors. This can enhance interpretability and reduce the risk of overfitting when dealing with high-dimensional datasets or when there are many\n",
    "# potentially irrelevant predictors.\n",
    "\n",
    "# On the other hand, if the focus is on overall model simplicity and shrinkage without necessarily eliminating any predictors, Ridge regularization (Model A) might be a better\n",
    "# choice. Ridge regression helps mitigate multicollinearity issues by shrinking correlated coefficients together, and it generally leads to smaller but non-zero coefficients\n",
    "# for all predictors.\n",
    "\n",
    "# It's important to note that the choice of regularization method has trade-offs and limitations. While Lasso regularization can perform feature selection, it may struggle in \n",
    "# the presence of highly correlated predictors (multicollinearity) and arbitrarily select one predictor while shrinking others, leading to instability in feature selection \n",
    "# results. Ridge regularization, while addressing multicollinearity, does not perform explicit feature selection and retains all predictors, which may not be desirable in some\n",
    "# scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595cdc8e-b62b-4d34-bdd6-e4d2ebcd2e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 12\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d8550-38c3-4562-9593-60b33ccee803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
